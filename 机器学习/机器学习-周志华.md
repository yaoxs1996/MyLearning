[TOC]

# 第1章 绪论
## 1.1 引言
## 1.2 基于术语
_数据集_  
_示例或样本_  
_属性或特性_  
_属性值_  
_属性空间、样本空间或输入空间_  
_特征向量_：空间中每个点对应一个坐标向量  
$D=\{x_1,x_2,...,x_m\}$表示包含m个示例的数据集。  
每个示例有d个属性描述，$\vec x_i=(x_{i1};x_{i2};...;x_{id})$。  
预测离散值 分类  
预测连续值 回归  
## 1.3 假设空间
归纳（induction），“泛化”（generalization）  
演绎（deduction），“特化”（specialization）  
将学习过程看做一个在所有假设组成的空间中进行搜索的过程，搜索目标是找到与训练集匹配的假设。  
## 1.4 归纳偏好
* __归纳偏好__：机器学习算法在学习过程中对某种类型假设的偏好。  
任何有效的机器学习算法必有其归纳偏好。  
* 引导算法确立偏好的一般性原则：__奥卡姆剃刀__ ，若有多个假设与观察一致，则选最简单的那个。  
* __没有免费午餐__ 定理（NFL定理）：所有问题出现机会相同、或所有问题同等重要的前提下，所有学习算法期望值相同。  
谈论算法的优劣，必须针对具体的学习问题。  
## 1.5 发展历程
## 1.6 应用现状

---
# 第2章 模型评估与选择
## 2.1 经验误差与过拟合
* 训练集上的误差 _训练误差_ 或 _经验误差_。  
* 新样本上的误差 _泛化误差_。  
* 过拟合是ML的关键障碍。
* ML面临NP难。  
## 2.2 评估方法
从数据集中分出训练集和测试集。  
### 2.2.1 留出法
直接将数据基划分为两个互斥的集合。  
尽量保持数据分布的一致性。  
___一般采用若干次随机划分、重复进行实验评估后取平均值作留出法的评估结果___。  
### 2.2.2 交叉验证法
* 数据集划分成k个大小相似的互斥子集。  
* 每个子集尽可能保持数据分布的一致性。  
* 每次用k-1个子集作为训练集、余下的那个子集作为测试集。  
* 最终返回k个测试结果的均值。  
### 2.2.3 自助法
* 以自助采样法为基础。  
* 对数据集D采样生成数据集D'：
    * 每次从D中挑选一个样本，将其拷贝放入D'中，然后再将该样本放回初始数据集D中。  
    * 该过程重复m次，就获得了包含m个样本的D'。
* D中约有36.8%的样本未出现在采样数据集D'中。
* 用D'做测试集。
* 改变了数据集的分布，产生了估计偏差。  
### 2.2.4 调参与最终模型
## 2.3 性能度量
### 2.3.1 错误率与精度
### 2.3.2 查准率、查全率与F1
* 分类结果混淆矩阵

| 真实情况\预测结果 | 正例 | 反例 |
| - | - | - |
| 正例 | TP（真正例） | FN（假反例） |
| 反例 | FP（假正例） | TN（真反例） |
* 查准率P，检索出的信息有多少是用户关心的：  
$$P={TP \over {TP+FP}}$$
* 查全率R，用户感兴趣的信息有多少被检索出来了：  
$$R={TP \over {TP+FN}}$$
* 查准率-查全率曲线 P-R图
学习器A的P-R曲线完全包住了学习器B的曲线，可以认为A的性能高于B。  
平衡点：查准率=查全率时的取值。  
* F1度量：
$$F1={{2 \times P \times R} \over {P+R}}={{2 \times TP} \over {样本总数+TP-TN}}$$
F1度量的一般形式$F_\beta$，定义：
$$F_\beta={{(1+\beta^2) \times P \times R} \over {(\beta^2 \times P)+R}}$$
其中$\beta>0$。$\beta=1$时退化为标准F1；$\beta>1$时，查全率有更大影响；$\beta<1$时，查准率有更大影响。  
### 2.3.3 ROC与AUC
* ROC受试者工作特性
ROC曲线使用真正例率TPR-假正例率FPR：  
$$TPR={{TP} \over {TP+FN}}$$
$$FPR={{FP} \over {TN+FP}}$$
比较学习器的性能，比较ROC曲线下的面积，即AUC（Area Under ROC Curve）。  
### 2.3.4 代价敏感错误率与代价曲线
非均等代价：权衡不同类型错误所造成的不同损失。  
## 2.4 比较检验
### 2.4.1 假设检验

* 二项检验
* t检验

### 2.4.2 交叉验证t检验

### 2.4.3 McNemar检验

检验两学习器的分类结果的差别。

### 2.4.4 Friedman检验与Nemenyi后续检验

在一组数据集上比较多个算法。  

* 基于算法排序的Friedman检验
    * 使用留出法或交叉验证法得到每个算法在每个数据集上的测试结果；
    * 根据性能进行排序；
    * 测试性能相同则平分序值；
    * 使用Friedman检验判断算法性能是否相同。
    
若算法性能显著不同，使用后续检验来进一步区分。常用Nemenyi后续检验。  
Nemenyi检验计算出平均序值差别的临界值域  

$$CD={q_\alpha{\sqrt {{k(k+1)} \over {6N}}}}$$

若两算法的平均序值之差超出临界值域CD，则以相应的置信度拒绝“两个算法性能相同”这一假设。  

## 2.5 偏差与方差

_偏差-方差分解_ 是解释学习算法泛化性能的一种重要工具。  
泛化误差可以分解成偏差、方差和噪声之和。  
_偏差_ 度量了学习算法本身的拟合能力；  
_方差_ 刻画了数据扰动所造成的影响；  
_噪声_ 刻画了学习问题本身的难度。  

---

# 第3章 线性模型

## 3.1 基本形式

给定由d个属性描述的$\vec x=(x_1;x_2;...;x_d)$  
线性模型视图学得一个通过属性的线性组合进行预测的函数：

$$f(\vec x)=w_1x_1+w_2x_2+...+w_dx_d+b$$

向量形式：

$$f(\vec x)=\vec w^T \vec x+b$$

其中$\vec w=(w_1;w_2;...;w_d)$。$\vec w$和b学得后即可确定模型。

## 3.2 线性回归

_线性回归_ 试图学得一个线性模型，尽可能准确预测实值输出标记。  
对于离散属性：  
若属性值存在 _序_ 的关系，通过连续化将其转化为连续值；  
若不存在序关系，则通常转化为k维向量。  
确定w和b，试图使均方误差最小化：  

$$(w^*,b^*)=arg\ min{\sum^{m}_{i=1}{(f(x_i)-y_i)^2}}$$

_最小二乘法_ 试图找到所有样本到直线的欧式距离之和最小的一条直线。  

## 3.3 对数几率回归

对于分类任务，找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。  
对于二分类任务，将实值z转换成0/1值。 _单位跃迁函数_ ：

$$y=\begin{cases}
0,\ z<0; \\
0.5,\ z=0;\\
1,\ z>0,
\end{cases}$$

单位跃迁函数不连续，使用对数几率函数替代：

$$y={1 \over {1-e^{-z}}}$$

对数几率回归，是一种分类方法，还可以近似概率预测。  

## 3.4 线性判别分析

线性判别分析（Linear Discriminant Analysis，LDA）一种经典的线性学习方法。  
思想：给定训练样例集，设法将样例投影到一条直线上，使同类样例的投影点尽可能接近、异类样例的投影尽可能远离；对新样本进行分类时，将其投影到同样这条直线上，再根据投影点的位置来确定新样本的类别。  
LDA也被视为一种经典的监督降维技术。  

## 3.5 多分类技术

先对问题进行拆分，对每个拆分出来的任务训练一个分类器；测试时，对分类器的预测结果进行集成获得最终的多分类结果。  
经典的三种拆分策略： _一对一_ （OvO）、 _一对其余_ （OvR）、 _多对多_ （MvM）。  

* OvO将N个类别两两配对，从而产生N(N-1)/2个二分类任务。
* OvR每次将一个类的样例作为正例、所有其他类作为反例来训练N个分类器。
* MvM每次将若干个类作为正类，若干个其他类作为反类。 _纠错输出码_

## 3.6 类别不平衡问题

分类任务中不同类别的训练样例数目差别很大的情况。  
令$m^+$表示正例数目，$m_-$表示反例数目，只要分类器的预测几率高于观测几率就应当判定为正例：  

$${y \over {1-y}}>{{m^+} \over {m^-}}$$

_再缩放_：

$${y' \over {1-y'}}={y \over {1-y}} \times {m^- \over m^+}$$

使得 _训练集是真实样本的无偏采样_ 有三类技术：  

* _欠采样_ ：丢弃部分反例
* _过采样_ ：增加部分正例
* _阈值移动_ ：使用原始训练集，进行预测时，嵌入再缩放公式

---

# 第4章 决策树

## 4.1 基本流程

一颗决策树包含一个根结点、若干个内部结点和若干个叶结点；  
叶结点对应决策结果，其他结点对应一个属性测试；  
每个结点包含的样本集合根据属性测试结果被划分到子结点中；  
根结点包含样本全集。  
决策树的生成是递归过程。三种情况导致递归返回：  

* 当前结点包含的样本属于同一类别
* 当前属性集为空，或所有样本的所有属性取值相同
* 当前结点包含的样本为空

## 4.2 划分选择

希望结点的 _纯度_ 越来越高。

### 4.2.1 信息增益

当前样本集合D中第k类样本所占的比例为$p_k$（k=1,2,...,|y|），则D的信息熵定义为：  

$$Ent(D)=-\sum^{|y|}_{k=1}{p_k\log_2 p_k}$$

Ent(D)的值越小，则D的纯度越高。  
第v个分支结点包含了D中所有在属性a上取值为$a^v$的样本，记为$D^v$。信息增益：  

$$Gain(D,a)=Ent(D)-{\sum^V_{v=1}{{|D^v|} \over {|D|}}Ent(D^v)}$$

一般而言，信息增益越大，使用属性a进行划分所得的纯度提升越大。  

### 4.2.2 增益率

信息增益准则对可取值数目较多的属性有所偏好。  
不直接使用信息增益，而是 _增益率_ 来选择最优划分属性。  
增益率定义：  

$$Gain\_ratio(D,a)={Gain(D,a) \over IV(a)}$$

其中IV(a)成为属性a的固有值。属性a的可能取值数目越多，IV(a)的值通常就越大。  

$$IV(a)=-{\sum^V_{v=1}{{|D^v|} \over {|D|}}\log_2{|D^v|\over|D|}}$$

增益率准则对可取值数目较小的属性有所偏好。

### 4.2.3 基尼指数

数据集D的纯度可以用 _基尼值_ 来度量：  

$$Gini(D)={\sum^{|y|}_{k=1}\sum_{k'\neq k}p_kp_{k'}}=1-{\sum^{|y|}_{k=1}p^2_k}$$

Gini(D)越小，数据集D的纯度越高。  
属性a的基尼指数定义：  

$$Gini\_index(D,a)={\sum^V_{v=1}{|D^v|\over|D|}Gini(D^v)}$$

## 4.3 剪枝处理

决策树学习算法对付过拟合的主要手段。  
基本策略 _预剪枝_ 和 _后剪枝_。  
_预剪枝_：决策树生成过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化能力的提升，则停止划分当前结点，并标记为叶结点。  
_后剪枝_：自底向上对非叶结点进行考察，若当前结点对应的子树被替换成叶结点能带来泛化能力的提升，则进行替换。  

### 4.3.1 预剪枝

基于信息增益准则，验证集精度。  

### 4.3.2 后剪枝

## 4.4 连续与缺失值

### 4.4.1 连续值处理

最简单的策略是采用 _二分法_ 处理连续属性。  
基于划分点t将D分为子集$D_t^-$和$D_t^+$。  

$$Gain(D,a)=\max \limits_{t \in {T_a}}Gain(D,a,t)$$

### 4.4.2 缺失值处理

* _如何在属性值缺失的情况下，进行划分属性选择_
给定训练集D和属性a，令$\tilde{D}$表示D中属性a上没有缺失值的样本子集。  
仅使用$\tilde{D}$来判断属性优劣。  
* _给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分_
若样本x在划分属性a上的取值未知，则将x同时划入所有的子结点，且样本权值与属性值$a^v$对应的子结点中调整为$\tilde{r}_v\cdot{w_x}$。即 ___让同一个样本以不同的概率划分到不同的子结点中去___。

## 4.5 多变量决策树

决策树形成的分类边界有一个明显的特点： _轴平行_，即它的分类边界由若干个与坐标轴平行的分段组成。  
当划分段数过多时，决策树会相当复杂。  
采用斜的划分边界能大为简化模型。  
__多变量决策树__ 就是实现这样的 _斜划分_ 甚至更复杂划分的决策树。  

---

# 第5章 神经网络

## 5.1 神经元模型

_M-P神经元模型_：神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将于神经元的阈值进行比较，通过 _激活函数_ 处理以产生神经元的输出。  
理想的激励函数是阶跃函数，实际常用Sigmoid函数。  

## 5.2 感知机与多层网络

感知机有两层神经元组成。能够容易地实现与、或、非运算。  
感知机只有输出层神经元进行激活函数处理，学习能力有限，只能解决线性可分问题，即 _存在一个线性超平面能够将两类模式分开_，则感知机的学习过程一定会收敛。  
解决非线性可分问题，需要使用多层功能神经元。 _多层前馈神经网络_。  
神经网络学习到的东西蕴含在 ___连接权___ 与 ___阈值___ 中。  

## 5.3 误差逆传播算法

__误差逆传播__ 算法（error BackPropagation，BP）。  
BP算法不仅可以用于多层前馈神经网络，还可用于其他类型的神经网络。  
_BP网络_ 一般指用BP算法训练的多层前馈神经网络。  
给定训练集$D=\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\},x_i\in\R^d,y_i\in\R^l$，即输入示例由d个属性描述，输出l维实值向量。  
BP算法基于梯度下降策略（gradient descent），以目标的负梯度方向对参数进行调整。  
只需一个包含足够多的神经元的隐层，多层前馈神经网络就能够以任意精度逼近任意复杂度的连续函数。  

## 5.4 全局最小与局部最小

_局部极小_ 和 _全局最小_  
基于梯度的搜索是使用最为广泛的参数寻优方法。梯度下降法是沿着负梯度方向搜索最优解。  
使用以下策略跳出局部最小：  

* 以多组不同参数数值初始化多个神经网络，取误差最小的解作为最终参数。
* _模拟退火_ 技术
* 随机梯度下降
* 遗传算法

## 5.5 其他常见神经网络

### 5.5.1 RBF网络
RBF（Radial Basis Function，径向基函数）网络。一种单隐层前馈神经网络。  
使用径向基函数作为隐层神经元激活函数，输出层是对隐层神经元输出的线性组合。  
假定输入d维向量$\vec{x}$，输出为实值，RBF网络表示为  

$$\varphi(\vec{x})=\sum^{q}_{i=1}{w_i\rho(\vec{x},c_i)}$$

其中$q$为隐层神经元个数，$c_i$和$w_i$分别为第$i$个隐层神经元对应的中心和权重，$\rho(\vec{x},c_i)$是径向基函数。  

### 5.5.2 ART网络

竞争性学习是神经网络中一种无监督学习策略。  
该策略中，网络的输出神经元相互竞争，每时刻仅有一个神经元被激活，其他神经元的状态被抑制。_胜者通吃_ 原则。  
ART（Adaptive Resonance Theory，自适应谐振理论）网络是竞争学习的重要代表。  
该网络由比较层、识别层、识别阈值和重置模块构成。  

### 5.5.3 SOM网络

SOM（Self-Organizing Map，自组织映射）网络，一种竞争学习型的无监督神经网络。  

### 5.5.4 级联相关网络

结构自适应网络将网络结构也当做学习目标之一，并希望能在训练过程中找到最符合数据特点的网络结构。  
级联相关网络是结构自适应网络的重要代表。  

### 5.5.5 Elman网络

_递归神经网络_ 允许网络中出现环状结构，从而让一些神经元的输出反馈回来作为输入信号。  
Elman网络是最常用的递归神经网络之一。  

### 5.5.6 Boltzmann机

神经网络中有一类模型是为了网络状态定义一个 _能量_，能量最小化时，网络达到理想状态，网络的训练就是在最小化这个能量函数。  
Boltzmann机就是一种 _基于能量的模型_。  

## 5.6 深度学习

典型的深度学习模型就是很深的神经网络。  
增加隐层的数目。  

* _无监督逐层训练_
多隐层网络训练的有效手段。_预训练_ + _微调_。  
* _权共享_
节省训练开销的策略。让一组神经元使用相同的连接权。  

无论是DBN还是CNN，通过多层处理，逐渐将初始的低层特征表示转换为高层特征表示，用简单模型即完成复杂的分类等学习任务。  
可将深度学习理解为 _特征学习_ 或 _表示学习_。  

---

# 第6章 支持向量机

## 6.1 间隔与支持向量

选取的划分超平面产生的分类结果是最鲁棒的，对于未见示例的泛化能力最强。  
划分超平面通过如下的线性方程来描述：  

$$\vec{w}^T\vec{x}+b=0$$

$\vec{w}=(w_1;w_2;...;w_d)$为法向量；b为位移项。  
两个异类支持向量到超平面的距离之和称为 _间隔_。  

## 6.2 对偶问题

支持向量机的一个重要性质：训练完成后，大部分样本都不需要保留，最终样本仅与支持向量有关。  

## 6.3 核函数

原始样本空间也许并不存在一个正确划分两类样本的超平面。  
如果原始样本空间是有限维，那么一定存在一个高维特征空间使得样本可分。  
令$\phi(\vec{x})$表示$\vec{x}$映射后的特征向量，在特征空间划分超平面对应的模型可以表示为  

$$f(\vec{x})=\vec{w}^T\phi(\vec{x})+b$$

求解后  

$$
\begin{aligned}
f(\vec{x})&=\vec{w}^T\phi(\vec{x})+b\\
&=\sum_{i=1}^{m}\alpha_iy_i\phi(\vec{x}_i)^T\phi(\vec{x})+b\\
&=\sum_{i=1}^{m}\alpha_iy_i\kappa(\vec{x},\vec{x}_i)+b
\end{aligned}
$$

函数$\kappa(.,.)$就是 _核函数_。
__定理（核函数）__ 令$\chi$为输入空间，$\kappa(.,.)$是定义在$\chi\times\chi$上对称函数，则$\kappa$是核函数当且仅当对于任意数据$D=\{\vec{x}_1,\vec{x}_2,...,\vec{x}_m\}$，_核矩阵_ K总是半正定的：  

$$
K=\begin{bmatrix}
   \kappa(\vec{x}_1,\vec{x}_1) & \dots & \kappa(\vec{x}_1,\vec{x}_j) & \dots & \kappa(\vec{x}_1,\vec{x}_m)\\
   \vdots & \ddots & \vdots & \ddots & \vdots\\
   \kappa(\vec{x}_i,\vec{x}_1) & \dots & \kappa(\vec{x}_i,\vec{x}_j) & \dots & \kappa(\vec{x}_i,\vec{x}_m)\\
   \vdots & \ddots & \vdots & \ddots & \vdots\\
   \kappa(\vec{x}_m,\vec{x}_1) & \dots & \kappa(\vec{x}_m,\vec{x}_j) & \dots & \kappa(\vec{x}_m,\vec{x}_m)
\end{bmatrix}
$$

## 6.4 软间隔与正则化

_软间隔_ 允许某些样本不满足约束  

$$y_i(\vec{w}^T\vec{x}_i+b)\geq1$$

最大化间隔的同时，不满足约束的样本应尽可能少。  
三种替代损失函数：  

* hinge损失：$l_{hinge}(z)=\max(0,1-z)$
* 指数损失：$l_{exp}(z)=\exp(-z)$
* 对率损失：$l_{\log}(z)=\log{(1+\exp{(-z)})}$

## 6.5 支持向量回归

_支持向量回归_ 假设我们能容忍$f(\vec{x})$与真实输出$y$之间最多有$\epsilon$的偏差。相当于以$f(\vec{x})$为中心，构建一个宽度为$2\epsilon$的间隔带。  

## 6.6 核方法

__定理（表示定理）__ 令$\Bbb{H}$为核函数$\kappa$对于的再生核希尔伯特空间，$\lVert{h}\rVert_{\Bbb{H}}$表示$\Bbb{H}$空间中关于$h$的范数，对于任意单调递增函数$\Omega：[0,\infin]\mapsto\R$和任意非负损失函数$l:\R^m\mapsto[0,\infin]$，优化问题  

$$\min\limits_{h\in\Bbb{H}}F(h)=\Omega(\lVert{h}\rVert_{\Bbb{H}})+l(h(\vec{x}_1),h(\vec{x}_2),...,h(\vec{x}_m))$$

的解总可写为  

$$h^*(\vec{x})=\sum_{i=1}^{m}\alpha_i\kappa(\vec{x},\vec{x}_i)$$

---

# 第7章 贝叶斯分类器

## 7.1 贝叶斯决策论

概率框架下实施决策的基本方法。对分类任务而言，在所有相关概率都已知的的理想情形下，贝叶斯决策论考虑如何基于这些概率和误判损失来选择最优的类别标记。  

## 7.2 极大似然估计

概率模型训练过程就是 _参数估计_。  
参数估计的解决方案之一：频率主义学派，参数虽然未知，但却是客观存在的固定值。极大似然估计。  

## 7.3 朴素贝叶斯分类器

_属性条件独立性假设_：对已知类别，假设所有属性相互独立。  
朴素贝叶斯分类器表达式：  

$$h_{nb}(\vec{x})=\argmax\limits_{c\in{y}}P(c)\prod^{d}_{i=1}P(x_i|c)$$

## 7.4 半朴素贝叶斯分类器

基本思想：适当考虑一部分属性间的相互依赖信息，从而既不需要进行完全联合概率计算，又不至于彻底忽略了比较强的属性依赖关系。  
_独依赖估计_（One-Dependent Estimator，ODE）是半朴素贝叶斯分类器最常用的一种策略。  
_独依赖_ 就是假设每个属性在类别之外最多依赖于一个其他属性。  

## 7.5 贝叶斯网

_贝叶斯网_ 亦称 _信念网_ ，借助有向无环图刻画属性之间的依赖关系，使用条件概率表来描述属性的联合概率分布。  
一个贝叶斯网$B$由结构$G$和参数$\Theta$两部分构成，$B=\langle{G,\Theta}\rangle$。  
网络结构$G$是一个有向无环图，每个结点对应一个属性；参数$\Theta$定量描述这种依赖关系。  

### 7.5.1 结构

给定父结点集，贝叶斯网假设每个属性与它的非后裔属性独立，于是$B=\langle{G,\Theta}\rangle$将属性$x_1,x_2,...,x_d$的联合概率分布定义为  

$$P_B(x_1,x_2,...,x_d)=\prod^{d}_{i=1}P_B(x_i|\pi_i)=\prod^{d}_{i=1}\theta_{x_i|\pi_i}$$

### 7.5.2 学习

贝叶斯网学习的首要任务就是根据训练数据集来找出结构最恰当的贝叶斯网。  
_评分搜索_ 是求解这一问题的常用办法。  

### 7.5.3 推断

贝叶斯网的近似推断使用吉布斯采样来完成。  

## 7.6 EM算法

_隐变量_ ：未观测变量。  
EM算法是常用语的估计参数隐变量的利器，迭代式的方法，基本思想：若参数$\Theta$已知，则可根据训练数据推断出最优隐变量$\Zeta$的值（E步）；反之，若$\Zeta$的值已知，则可方便对参数$\Theta$做极大似然估计（M步）。  
EM算法使用两个步骤交替计算：第一步是期望步（E）步，利用当前估计的参数值来计算对数似然的期望值；第二步是最大化（M）步，寻找能使E步产生的似然期望最大化的参数值。然后，新得到的参数值重新被用于E步，直至收敛到局部最优解。  

---

# 第8章 集成学习

## 8.1 个体与集成

_集成学习_ 通过构建并结合多个学习器来完成学习任务。  
同质集成， _基学习器_  
异质集成， _组件学习器_  
要想获得好的集成，个体学习器应当 _好而不同_，个体学习器要有一定的准确性，并且要有多样性，即学习器之间存在差异。  

## 8.2 Boosting

Boosting是一族可将弱学习器提升学习器的算法。  
从偏差-方差分解的角度看，Boosting主要关注降低偏差，因此Boosting能基于泛化能力相当弱的学习器构建出很强的集成。  

## 8.3 Bagging与随机森林

给定一个训练数据集，一种可能的做法就是对训练样本进行采样，产生若干个不同子集，再从每个数据子集中训练出一个基学习器。  

### 8.3.1 Bagging

Bagging是并行式集成学习方法的代表。  
直接基于自主采样法。  
Bagging的基本流程：采样出T个含有m个训练样本的采样集，基于每个采样集训练出一个基学习器，再将这些基学习器进行结合。  
Bagging对分类任务使用简单投票法，对回归任务使用简单平均法。  

### 8.3.2 随机森林

随机森林是Bagging的一个扩展变体。以决策树为基学习器构建Bagging集成的基础上，进一步在决策树的训练过程中引入随机属性选择。  
在RF中，对基决策树的每个结点，先从结点中随机选择一个包含k个属性的子集，然后再从这个子集中选择一个最优属性用于划分。  

## 8.4 结合策略

三个方面的好处：  

1. 统计的原因
2. 计算的原因
3. 表示的原因

### 8.4.1 平均法

对数值型输出$h_i(\vec{x})\in\R$，最常见的结合策略是使用平均法。  

* 简单平均法
* 加权平均法

一般而言，在个体学习器性能相差较大时，宜使用加权平均法，而在个体学习器性能相近时，宜使用简单平均法。  

### 8.4.2 投票法

对于分类任务来说，学习器$h_i$将从类别标记集合中预测出一个标记，常见的结合策略是使用投票法。  

* 绝对多数投票法
即若某标记得票过半数，则预测为该标记；否则拒绝预测。  
* 相对多数投票法
即预测为得票数最多的标记，若同时有多个标记获得最高票，则从中随机选择一个。
* 加权投票法

### 8.4.3 学习法

通过另一个学习器来进行结合。 _Stacking_ 是学习法的典型代表。  
把个体学习器称为初级学习器，用于结合的学习器称为次级学习器。  

## 8.5 多样性

### 8.5.1 误差-分歧分解

_误差-分歧分解_：个体学习器准确性越高、多样性越大，则集成越好。  

###  8.5.2 多样性度量

典型做法是考虑个体分类器的两两相似/不相似性。  
常见的多样性度量：  

* 不合度量
* 相关系数
* Q-统计量
* $\kappa$-统计量

### 8.5.3 多样性增强

* 数据样本扰动
* 输入属性扰动
* 输出表示扰动
* 输出表示扰动
* 算法参数扰动

---

# 第9章 聚类

## 9.1 聚类任务

假定样本集$D=\{\vec{x}_1,\vec{x}_2,...,\vec{x}_m\}$包含$m$个无标记样本，每个样本$\vec{x}_i=(x_{i1};x_{i2};...;x_{in})$是一个n维特征向量，则聚类算法将样本集D划分为$k$个不相交的簇。  

## 9.2 性能度量

_有效性指标_  
_簇内相似性_ 高且 _簇间相似度_ 低。  

* 将聚类结果与某一参考模型进行比较，称为 _外部指标_
    - Jaccard系数
    - FM指数（Fowlkes and Mallows Index，FMI）
    - Rand指数
* 直接考察聚类结果，称为 _内部指标_
    - DB指数（Davies-Bouldin Index，简称DBI）
    - Dunn指数（Dunn Index，DI）

## 9.3 距离计算

对函数$dist(.,.)$，距离度量，满足一些基本性质：  

* 非负性：$dist(\vec{x}_i,\vec{x}_j)\geq0$
* 同一性：$dist(\vec{x}_i,\vec{x}_j)=0$当且仅当$\vec{x}_i=\vec{x}_j$
* 对称性
* 直递性

_闵可夫斯基距离_  

$$dist_{mk}(\vec{x}_i,\vec{x}_j)=(\sum^{n}_{u=1}|x_{iu}-x_{ju}|^p)^{1\over{p}}$$

$p=2$时，闵可夫斯基距离即欧氏距离  
$p=1$时，闵可夫斯基距离即曼哈顿距离  
闵可夫斯基距离可用于有序属性。对于无序属性，可采用VDM（Value Difference Metric）。  
将闵可夫斯基距离与VDM结合即可处理混合属性。  

## 9.4 原型聚类

_基于原型的聚类_ 假设聚类结构通过一组原型刻画。  

### 9.4.1 k均值算法

给定样本集$D=({\vec{x}_1,\vec{x}_2,...,\vec{x}_m})$，k均值算法针对所得簇划分$c=\{C_1,C_2,...,C_k\}$最小化平方误差  

$$E=\sum^{k}_{i=1}\sum_{\vec{x}\in{C_i}}\lVert{\vec{x}-\vec{\mu_i}}\rVert_2^2$$

其中$\vec\mu_i={1\over{|C_i|}}\sum_{x\in{C_i}}\vec{x}$是簇$C_i$的均值向量。  
k均值算法采用贪心策略，通过迭代优化来近似求解。  

### 9.4.2 学习向量量化

_学习向量量化_ 试图找到一组原型向量来刻画聚类结构，LVQ假设数据样本带有类别标记，学习过程利用这些样本的监督信息来辅助聚类。  

### 9.4.3 高斯混合聚类

采用概率模型来表达聚类原型。

## 9.5 密度聚类

此类算法假设聚类结构能够通过样本分布的紧密程度确定。密度聚类算法从样本密度的角度来考察样本之间的可连接性，并基于连接样本不断拓展聚类簇以获得最终的效果。  
DBSCAN是一种著名的密度聚类算法，基于一组邻域参数$(\epsilon,MinPts)$来刻画样本分布的紧密程度。  
给定数据集$D=\{\vec{x}_1,\vec{x}_2,...,\vec{x}_m\}$，定义一下几个概念：  

* $\epsilon$邻域：对$\vec{x}_j\in{D}$，其$\epsilon$-邻域包含样本集D中与$\vec{x}_j$的距离不大于$\epsilon$的样本。
* 核心对象：若$\vec{x}_j$的$\epsilon$-邻域至少包含$MinPts$个样本，则$\vec{x}_j$是一个核心对象。
* 密度直达：若$\vec{x}_j$位于$\vec{x}_i$的$\epsilon$-邻域中，且$\vec{x}_i$是核心对象，则称$\vec{x}_j$由$\vec{x}_i$密度直达。
* 密度可达：对$\vec{x}_i$与$\vec{x}_j$，若存在样本序列$\vec{p}_1,\vec{p}_2,...,\vec{p}_n$，其中$\vec{p}_1=\vec{x}_i$，$\vec{p}_n=\vec{x}_j$且$\vec{p}_{i+1}$由$\vec{p}_i$密度直达，则称$\vec{x}_j$由$\vec{x}_i$密度可达。
* 密度相连：对$\vec{x}_i$与$\vec{x}_j$，若存在$\vec{x}_k$使得$\vec{x}_i$与$\vec{x}_j$均由$\vec{x}_k$密度可达，则称$\vec{x}_i$与$\vec{x}_j$密度相连。

DBSCAN将簇定义为：由密度可达关系导出的最大的密度相连样本集合。  

## 9.6 层次聚类

试图在不同层次对数据集进行划分，从而形成树形的聚类结构。  
数据集的划分可采用 _自底向上_ 的聚合策略，也可采用 _自顶向下_ 的分拆策略。  

---

# 第10章 降维与度量学习

## 10.1 k近邻学习

k近邻（kNN）学习是一种常见的监督学习方法。  
工作机制：给定测试样本，基于某种距离度量找出训练集中与其最靠近的k个训练样本，然后基于这k个邻居的信息来进行预测。  
在分类任务中使用投票法，在回归任务中使用平均法。  
kNN是 _懒惰学习_ 的著名代表。在训练阶段仅仅是把样本保存起来，训练时间开销为零，待收到测试样本再进行处理。  

## 10.2 低维嵌入

高维情形下出现的数据样本稀疏、距离计算困难等问题，是所有机器学习面临的严重障碍，称为 _维数灾难_。
_降维_ 或 _维数约简_。  
若要求原始空间中样本之间的距离在低维空间中得以保持，得到 _多维缩放_。  

## 10.3 主成分分析

一种常用的降维方法。  
降维导致部分特征向量被舍弃，这是降维的必然结果，也是必要的  

* 增大采样密度
* 去噪

## 10.4 核化线性降维


