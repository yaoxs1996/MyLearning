[TOC]

# 第1章 绪论
## 1.1 引言
## 1.2 基于术语
_数据集_  
_示例或样本_  
_属性或特性_  
_属性值_  
_属性空间、样本空间或输入空间_  
_特征向量_：空间中每个点对应一个坐标向量  
$D=\{x_1,x_2,...,x_m\}$表示包含m个示例的数据集。  
每个示例有d个属性描述，$\vec x_i=(x_{i1};x_{i2};...;x_{id})$。  
预测离散值 分类  
预测连续值 回归  
## 1.3 假设空间
归纳（induction），“泛化”（generalization）  
演绎（deduction），“特化”（specialization）  
将学习过程看做一个在所有假设组成的空间中进行搜索的过程，搜索目标是找到与训练集匹配的假设。  
## 1.4 归纳偏好
* __归纳偏好__：机器学习算法在学习过程中对某种类型假设的偏好。  
任何有效的机器学习算法必有其归纳偏好。  
* 引导算法确立偏好的一般性原则：__奥卡姆剃刀__ ，若有多个假设与观察一致，则选最简单的那个。  
* __没有免费午餐__ 定理（NFL定理）：所有问题出现机会相同、或所有问题同等重要的前提下，所有学习算法期望值相同。  
谈论算法的优劣，必须针对具体的学习问题。  
## 1.5 发展历程
## 1.6 应用现状

---
# 第2章 模型评估与选择
## 2.1 经验误差与过拟合
* 训练集上的误差 _训练误差_ 或 _经验误差_。  
* 新样本上的误差 _泛化误差_。  
* 过拟合是ML的关键障碍。
* ML面临NP难。  
## 2.2 评估方法
从数据集中分出训练集和测试集。  
### 2.2.1 留出法
直接将数据基划分为两个互斥的集合。  
尽量保持数据分布的一致性。  
___一般采用若干次随机划分、重复进行实验评估后取平均值作留出法的评估结果___。  
### 2.2.2 交叉验证法
* 数据集划分成k个大小相似的互斥子集。  
* 每个子集尽可能保持数据分布的一致性。  
* 每次用k-1个子集作为训练集、余下的那个子集作为测试集。  
* 最终返回k个测试结果的均值。  
### 2.2.3 自助法
* 以自助采样法为基础。  
* 对数据集D采样生成数据集D'：
    * 每次从D中挑选一个样本，将其拷贝放入D'中，然后再将该样本放回初始数据集D中。  
    * 该过程重复m次，就获得了包含m个样本的D'。
* D中约有36.8%的样本未出现在采样数据集D'中。
* 用D'做测试集。
* 改变了数据集的分布，产生了估计偏差。  
### 2.2.4 调参与最终模型
## 2.3 性能度量
### 2.3.1 错误率与精度
### 2.3.2 查准率、查全率与F1
* 分类结果混淆矩阵

| 真实情况\预测结果 | 正例 | 反例 |
| - | - | - |
| 正例 | TP（真正例） | FN（假反例） |
| 反例 | FP（假正例） | TN（真反例） |
* 查准率P，检索出的信息有多少是用户关心的：  
$$P={TP \over {TP+FP}}$$
* 查全率R，用户感兴趣的信息有多少被检索出来了：  
$$R={TP \over {TP+FN}}$$
* 查准率-查全率曲线 P-R图
学习器A的P-R曲线完全包住了学习器B的曲线，可以认为A的性能高于B。  
平衡点：查准率=查全率时的取值。  
* F1度量：
$$F1={{2 \times P \times R} \over {P+R}}={{2 \times TP} \over {样本总数+TP-TN}}$$
F1度量的一般形式$F_\beta$，定义：
$$F_\beta={{(1+\beta^2) \times P \times R} \over {(\beta^2 \times P)+R}}$$
其中$\beta>0$。$\beta=1$时退化为标准F1；$\beta>1$时，查全率有更大影响；$\beta<1$时，查准率有更大影响。  
### 2.3.3 ROC与AUC
* ROC受试者工作特性
ROC曲线使用真正例率TPR-假正例率FPR：  
$$TPR={{TP} \over {TP+FN}}$$
$$FPR={{FP} \over {TN+FP}}$$
比较学习器的性能，比较ROC曲线下的面积，即AUC（Area Under ROC Curve）。  
### 2.3.4 代价敏感错误率与代价曲线
非均等代价：权衡不同类型错误所造成的不同损失。  
## 2.4 比较检验
### 2.4.1 假设检验

* 二项检验
* t检验

### 2.4.2 交叉验证t检验

### 2.4.3 McNemar检验

检验两学习器的分类结果的差别。

### 2.4.4 Friedman检验与Nemenyi后续检验

在一组数据集上比较多个算法。  

* 基于算法排序的Friedman检验
    * 使用留出法或交叉验证法得到每个算法在每个数据集上的测试结果；
    * 根据性能进行排序；
    * 测试性能相同则平分序值；
    * 使用Friedman检验判断算法性能是否相同。
    
若算法性能显著不同，使用后续检验来进一步区分。常用Nemenyi后续检验。  
Nemenyi检验计算出平均序值差别的临界值域  

$$CD={q_\alpha{\sqrt {{k(k+1)} \over {6N}}}}$$

若两算法的平均序值之差超出临界值域CD，则以相应的置信度拒绝“两个算法性能相同”这一假设。  

## 2.5 偏差与方差

_偏差-方差分解_ 是解释学习算法泛化性能的一种重要工具。  
泛化误差可以分解成偏差、方差和噪声之和。  
_偏差_ 度量了学习算法本身的拟合能力；  
_方差_ 刻画了数据扰动所造成的影响；  
_噪声_ 刻画了学习问题本身的难度。  

---

# 第3章 线性模型

## 3.1 基本形式

给定由d个属性描述的$\vec x=(x_1;x_2;...;x_d)$  
线性模型视图学得一个通过属性的线性组合进行预测的函数：

$$f(\vec x)=w_1x_1+w_2x_2+...+w_dx_d+b$$

向量形式：

$$f(\vec x)=\vec w^T \vec x+b$$

其中$\vec w=(w_1;w_2;...;w_d)$。$\vec w$和b学得后即可确定模型。

## 3.2 线性回归

_线性回归_ 试图学得一个线性模型，尽可能准确预测实值输出标记。  
对于离散属性：  
若属性值存在 _序_ 的关系，通过连续化将其转化为连续值；  
若不存在序关系，则通常转化为k维向量。  
确定w和b，试图使均方误差最小化：  

$$(w^*,b^*)=arg\ min{\sum^{m}_{i=1}{(f(x_i)-y_i)^2}}$$

_最小二乘法_ 试图找到所有样本到直线的欧式距离之和最小的一条直线。  

## 3.3 对数几率回归

对于分类任务，找一个单调可微函数将分类任务的真实标记y与线性回归模型的预测值联系起来。  
对于二分类任务，将实值z转换成0/1值。 _单位跃迁函数_ ：

$$y=\begin{cases}
0,\ z<0; \\
0.5,\ z=0;\\
1,\ z>0,
\end{cases}$$

单位跃迁函数不连续，使用对数几率函数替代：

$$y={1 \over {1-e^{-z}}}$$

对数几率回归，是一种分类方法，还可以近似概率预测。  

## 3.4 线性判别分析

线性判别分析（Linear Discriminant Analysis，LDA）一种经典的线性学习方法。  
思想：给定训练样例集，设法将样例投影到一条直线上，使同类样例的投影点尽可能接近、异类样例的投影尽可能远离；对新样本进行分类时，将其投影到同样这条直线上，再根据投影点的位置来确定新样本的类别。  
LDA也被视为一种经典的监督降维技术。  

## 3.5 多分类技术

先对问题进行拆分，对每个拆分出来的任务训练一个分类器；测试时，对分类器的预测结果进行集成获得最终的多分类结果。  
经典的三种拆分策略： _一对一_ （OvO）、 _一对其余_ （OvR）、 _多对多_ （MvM）。  

* OvO将N个类别两两配对，从而产生N(N-1)/2个二分类任务。
* OvR每次将一个类的样例作为正例、所有其他类作为反例来训练N个分类器。
* MvM每次将若干个类作为正类，若干个其他类作为反类。 _纠错输出码_

## 3.6 类别不平衡问题

分类任务中不同类别的训练样例数目差别很大的情况。  
令$m^+$表示正例数目，$m_-$表示反例数目，只要分类器的预测几率高于观测几率就应当判定为正例：  

$${y \over {1-y}}>{{m^+} \over {m^-}}$$

_再缩放_：

$${y' \over {1-y'}}={y \over {1-y}} \times {m^- \over m^+}$$

使得 _训练集是真实样本的无偏采样_ 有三类技术：  

* _欠采样_ ：丢弃部分反例
* _过采样_ ：增加部分正例
* _阈值移动_ ：使用原始训练集，进行预测时，嵌入再缩放公式

---

# 第4章 决策树

## 4.1 基本流程

一颗决策树包含一个根结点、若干个内部结点和若干个叶结点；  
叶结点对应决策结果，其他结点对应一个属性测试；  
每个结点包含的样本集合根据属性测试结果被划分到子结点中；  
根结点包含样本全集。  
决策树的生成是递归过程。三种情况导致递归返回：  

* 当前结点包含的样本属于同一类别
* 当前属性集为空，或所有样本的所有属性取值相同
* 当前结点包含的样本为空

## 4.2 划分选择

希望结点的 _纯度_ 越来越高。

### 4.2.1 信息增益

当前样本集合D中第k类样本所占的比例为$p_k$（k=1,2,...,|y|），则D的信息熵定义为：  

$$Ent(D)=-\sum^{|y|}_{k=1}{p_k\log_2 p_k}$$

Ent(D)的值越小，则D的纯度越高。  
第v个分支结点包含了D中所有在属性a上取值为$a^v$的样本，记为$D^v$。信息增益：  

$$Gain(D,a)=Ent(D)-{\sum^V_{v=1}{{|D^v|} \over {|D|}}Ent(D^v)}$$

一般而言，信息增益越大，使用属性a进行划分所得的纯度提升越大。  

### 4.2.2 增益率

信息增益准则对可取值数目较多的属性有所偏好。  
不直接使用信息增益，而是 _增益率_ 来选择最优划分属性。  
增益率定义：  

$$Gain\_ratio(D,a)={Gain(D,a) \over IV(a)}$$

其中IV(a)成为属性a的固有值。属性a的可能取值数目越多，IV(a)的值通常就越大。  

$$IV(a)=-{\sum^V_{v=1}{{|D^v|} \over {|D|}}\log_2{|D^v|\over|D|}}$$

增益率准则对可取值数目较小的属性有所偏好。

### 4.2.3 基尼指数

数据集D的纯度可以用 _基尼值_ 来度量：  

$$Gini(D)={\sum^{|y|}_{k=1}\sum_{k'\neq k}p_kp_{k'}}=1-{\sum^{|y|}_{k=1}p^2_k}$$

Gini(D)越小，数据集D的纯度越高。  
属性a的基尼指数定义：  

$$Gini\_index(D,a)={\sum^V_{v=1}{|D^v|\over|D|}Gini(D^v)}$$

## 4.3 剪枝处理

决策树学习算法对付过拟合的主要手段。  
基本策略 _预剪枝_ 和 _后剪枝_。  
_预剪枝_：决策树生成过程中，对每个结点在划分前进行估计，若当前结点的划分不能带来决策树泛化能力的提升，则停止划分当前结点，并标记为叶结点。  
_后剪枝_：自底向上对非叶结点进行考察，若当前结点对应的子树被替换成叶结点能带来泛化能力的提升，则进行替换。  

### 4.3.1 预剪枝

基于信息增益准则，验证集精度。  

### 4.3.2 后剪枝

## 4.4 连续与缺失值

### 4.4.1 连续值处理

最简单的策略是采用 _二分法_ 处理连续属性。  
基于划分点t将D分为子集$D_t^-$和$D_t^+$。  

$$Gain(D,a)=\max \limits_{t \in {T_a}}Gain(D,a,t)$$

### 4.4.2 缺失值处理

* _如何在属性值缺失的情况下，进行划分属性选择_
给定训练集D和属性a，令$\tilde{D}$表示D中属性a上没有缺失值的样本子集。  
仅使用$\tilde{D}$来判断属性优劣。  
* _给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分_
若样本x在划分属性a上的取值未知，则将x同时划入所有的子结点，且样本权值与属性值$a^v$对应的子结点中调整为$\tilde{r}_v\cdot{w_x}$。即 ___让同一个样本以不同的概率划分到不同的子结点中去___。

## 4.5 多变量决策树

决策树形成的分类边界有一个明显的特点： _轴平行_，即它的分类边界由若干个与坐标轴平行的分段组成。  
当划分段数过多时，决策树会相当复杂。  
采用斜的划分边界能大为简化模型。  
__多变量决策树__ 就是实现这样的 _斜划分_ 甚至更复杂划分的决策树。  

---

# 第5章 神经网络

## 5.1 神经元模型

_M-P神经元模型_：神经元接收到来自n个其他神经元传递过来的输入信号，这些输入信号通过带权重的连接进行传递，神经元接收到的总输入值将于神经元的阈值进行比较，通过 _激活函数_ 处理以产生神经元的输出。  
理想的激励函数是阶跃函数，实际常用Sigmoid函数。  

## 5.2 感知机与多层网络

感知机有两层神经元组成。能够容易地实现与、或、非运算。  
感知机只有输出层神经元进行激活函数处理，学习能力有限，只能解决线性可分问题，即 _存在一个线性超平面能够将两类模式分开_，则感知机的学习过程一定会收敛。  
解决非线性可分问题，需要使用多层功能神经元。 _多层前馈神经网络_。  
神经网络学习到的东西蕴含在 ___连接权___ 与 ___阈值___ 中。  

## 5.3 误差逆传播算法

__误差逆传播__ 算法（error BackPropagation，BP）。  
BP算法不仅可以用于多层前馈神经网络，还可用于其他类型的神经网络。  
_BP网络_ 一般指用BP算法训练的多层前馈神经网络。  
给定训练集$D=\{(x_1,y_1),(x_2,y_2),...,(x_m,y_m)\},x_i\in\R^d,y_i\in\R^l$，即输入示例由d个属性描述，输出l维实值向量。  
BP算法基于梯度下降策略（gradient descent），以目标的负梯度方向对参数进行调整。  
只需一个包含足够多的神经元的隐层，多层前馈神经网络就能够以任意精度逼近任意复杂度的连续函数。  

## 5.4 全局最小与局部最小

_局部极小_ 和 _全局最小_  
基于梯度的搜索是使用最为广泛的参数寻优方法。梯度下降法是沿着负梯度方向搜索最优解。  
使用以下策略跳出局部最小：  

* 以多组不同参数数值初始化多个神经网络，取误差最小的解作为最终参数。
* _模拟退火_ 技术
* 随机梯度下降
* 遗传算法

## 5.5 其他常见神经网络

### 5.5.1 RBF网络
RBF（Radial Basis Function，径向基函数）网络。一种单隐层前馈神经网络。  
使用径向基函数作为隐层神经元激活函数，输出层是对隐层神经元输出的线性组合。  
假定输入d维向量$\vec{x}$，输出为实值，RBF网络表示为  

$$\varphi(\vec{x})=\sum^{q}_{i=1}{w_i\rho(\vec{x},c_i)}$$

其中$q$为隐层神经元个数，$c_i$和$w_i$分别为第$i$个隐层神经元对应的中心和权重，$\rho(\vec{x},c_i)$是径向基函数。  

### 5.5.2 ART网络

竞争性学习是神经网络中一种无监督学习策略。  
该策略中，网络的输出神经元相互竞争，每时刻仅有一个神经元被激活，其他神经元的状态被抑制。_胜者通吃_ 原则。  
ART（Adaptive Resonance Theory，自适应谐振理论）网络是竞争学习的重要代表。  
该网络由比较层、识别层、识别阈值和重置模块构成。  

### 5.5.3 SOM网络

SOM（Self-Organizing Map，自组织映射）网络，一种竞争学习型的无监督神经网络。  

### 5.5.4 级联相关网络

结构自适应网络将网络结构也当做学习目标之一，并希望能在训练过程中找到最符合数据特点的网络结构。  
级联相关网络是结构自适应网络的重要代表。  

### 5.5.5 Elman网络

_递归神经网络_ 允许网络中出现环状结构，从而让一些神经元的输出反馈回来作为输入信号。  
Elman网络是最常用的递归神经网络之一。  

### 5.5.6 Boltzmann机

神经网络中有一类模型是为了网络状态定义一个 _能量_，能量最小化时，网络达到理想状态，网络的训练就是在最小化这个能量函数。  
Boltzmann机就是一种 _基于能量的模型_。  

## 5.6 深度学习

典型的深度学习模型就是很深的神经网络。  
增加隐层的数目。  

* _无监督逐层训练_
多隐层网络训练的有效手段。_预训练_ + _微调_。  
* _权共享_
节省训练开销的策略。让一组神经元使用相同的连接权。  

无论是DBN还是CNN，通过多层处理，逐渐将初始的低层特征表示转换为高层特征表示，用简单模型即完成复杂的分类等学习任务。  
可将深度学习理解为 _特征学习_ 或 _表示学习_。  

---

# 第6章 支持向量机

## 6.1 间隔与支持向量

划分超平面通过如下的线性方程来描述：  

$$\vec{w}^T\vec{x}+b=0$$

$\vec{w}=(w_1;w_2;...;w_d)$为法向量；b为位移向。  
两个异类支持向量到超平面的距离之和称为 _间隔_。  

## 6.2 对偶问题


